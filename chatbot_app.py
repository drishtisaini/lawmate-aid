import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import os
import streamlit as st
from generate_complaint import generate_complaint_from_message
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# ğŸ” API setup
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
os.environ["OPENAI_API_KEY"] = "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"  # Replace with real key

# ğŸ¤– LLM setup
llm = ChatOpenAI(model_name="mistralai/mixtral-8x7b-instruct", temperature=0.6)

# ğŸ“„ Streamlit setup
st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
st.title("âš–ï¸ LawMate - AI Legal Assistant")

# ğŸ—‚ Complaint Type
complaint_types = [
    "Consumer Complaint", "Rent Dispute",
    "FIR / Police Complaint", "Workplace Harassment", "Refund Request"
]
selected_type = st.selectbox("Choose Complaint Type", complaint_types)
st.session_state["complaint_type"] = selected_type

# ğŸ’¾ Init session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_prompt" not in st.session_state:
    st.session_state.last_prompt = ""
if "response_done" not in st.session_state:
    st.session_state.response_done = False
if "draft_done" not in st.session_state:
    st.session_state.draft_done = False

# ğŸ§¾ Show chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ğŸ§  Chat Input
if prompt := st.chat_input("Ask a legal question..."):
    st.session_state.last_prompt = prompt
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        st.session_state.messages.append({"role": "assistant", "content": response.content})
        st.session_state.response_done = True
        st.session_state.draft_done = False
    except Exception as e:
        st.error(f"âŒ Error from assistant: {e}")
        st.stop()

# âœ… Show assistant reply if response is done
if st.session_state.response_done:
    with st.chat_message("assistant"):
        st.markdown(st.session_state.messages[-1]["content"])

        # ğŸ“„ Button to trigger complaint generation
        if st.button("ğŸ“„ Generate Complaint Draft") and not st.session_state.draft_done:
            with st.spinner("Generating complaint draft..."):
                try:
                    draft = generate_complaint_from_message(st.session_state.last_prompt)
                    st.session_state["draft_text"] = draft
                    st.session_state.draft_done = True
                except Exception as e:
                    st.error(f"âŒ Error generating draft: {e}")
                    st.stop()

# âœ… Show complaint draft if ready
if st.session_state.get("draft_done", False):
    with st.chat_message("assistant"):
        st.success("âœ… Complaint draft generated!")
        st.markdown("### ğŸ“„ Complaint Draft:")
        st.code(st.session_state["draft_text"], language="markdown")
        st.download_button(
            label="ğŸ“¥ Download Draft as .txt",
            data=st.session_state["draft_text"],
            file_name="complaint_draft.txt",
            mime="text/plain"
        )














# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI  # âœ… Updated import

# from langchain.schema import HumanMessage

# # ğŸ” Set proxy OpenAI endpoint
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"

# # ğŸ”— Initialize LLM
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # ğŸŒ Streamlit UI setup
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")

# # ğŸ”„ Session state
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # ğŸ§¾ Display chat history
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # ğŸ—£ User input
# if prompt := st.chat_input("Ask a legal question..."):
#     # Save user message
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # Generate assistant reply
#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)

#             # âœ… Generate Legal Draft (only shown after response)
#             if st.button("ğŸ“„ Generate Complaint Draft"):
#                 with st.spinner("Generating draft..."):
#                     draft = generate_complaint_from_message(prompt)
#                     st.success("âœ… Draft generated!")
#                     st.markdown("### ğŸ“„ Complaint Draft:")
#                     st.code(draft, language="markdown")


#         except Exception as e:
#             st.error(f"âŒ Error occurred: {e}")

#     st.session_state.messages.append({"role": "assistant", "content": response.content})









# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI
# from langchain.schema import HumanMessage

# # ğŸ›¡ï¸ Set up OpenRouter LLM endpoint
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"  # Replace with your actual OpenRouter key

# # ğŸ¤– Load the language model
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # ğŸ–¥ï¸ Setup Streamlit UI
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")

# # ğŸ”„ Initialize chat history
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # ğŸ’¬ Display chat history
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # âœï¸ Handle user input
# if prompt := st.chat_input("Ask a legal question..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)
#         except Exception as e:
#             response = None
#             st.error(f"âŒ Error: {e}")

#     if response:
#         st.session_state.messages.append({"role": "assistant", "content": response.content})
#         st.session_state.last_prompt = prompt  # âœ… Save last prompt for later draft generation

# # ğŸ“„ Enable draft generation after successful interaction
# if "last_prompt" in st.session_state:
#     if st.button("ğŸ“„ Generate Complaint Draft"):
#         with st.spinner("Generating draft..."):
#             try:
#                 draft = generate_complaint_from_message(st.session_state.last_prompt)
#                 st.success("âœ… Draft generated!")
#                 st.markdown("### ğŸ“„ Complaint Draft:")
#                 st.code(draft, language="markdown")
#             except Exception as e:
#                 st.error(f"âš ï¸ Error generating complaint: {e}")












# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI
# from langchain.schema import HumanMessage

# # ğŸ” API setup
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"  # Replace with valid key

# # ğŸ¤– Initialize LangChain LLM
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # ğŸŒ Streamlit page config
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")

# # ğŸ”„ Maintain session state
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# if "last_prompt" not in st.session_state:
#     st.session_state.last_prompt = ""

# if "show_generate_button" not in st.session_state:
#     st.session_state.show_generate_button = False

# # ğŸ“œ Show chat history
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # ğŸ§  Get user input
# if prompt := st.chat_input("Ask a legal question..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     st.session_state.last_prompt = prompt
#     st.session_state.show_generate_button = False

#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # ğŸ¤– Assistant response
#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)
#             st.session_state.show_generate_button = True
#         except Exception as e:
#             st.error(f"âŒ Error: {e}")
#             response = None

#     if response:
#         st.session_state.messages.append({"role": "assistant", "content": response.content})

# # ğŸ“ Draft Generator Button
# if st.session_state.show_generate_button and st.session_state.last_prompt:
#     if st.button("\ud83d\udcc4 Generate Complaint Draft"):
#         with st.spinner("Generating complaint draft..."):
#             try:
#                 draft = generate_complaint_from_message(st.session_state.last_prompt)
#                 st.success("\u2705 Draft generated!")
#                 st.markdown("### \ud83d\udcc4 Complaint Draft:")
#                 st.code(draft, language="markdown")
#             except Exception as e:
#                 st.error(f"\u274c Error while generating draft: {e}")





# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI
# from langchain.schema import HumanMessage

# # Set proxy endpoint
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"  # ğŸ” Replace with your actual key

# # Initialize model
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # Streamlit page config
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")

# # Setup session state for chat
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# if "last_prompt" not in st.session_state:
#     st.session_state.last_prompt = ""

# # Display past messages
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # Capture new input
# if prompt := st.chat_input("Ask a legal question..."):
#     st.session_state.last_prompt = prompt
#     st.session_state.messages.append({"role": "user", "content": prompt})
    
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)
#             st.session_state.messages.append({"role": "assistant", "content": response.content})
#         except Exception as e:
#             st.error(f"âŒ Chat Error: {e}")

# # Complaint Draft Generation (shown always for now)
# if st.session_state.last_prompt:
#     if st.button("ğŸ“„ Generate Complaint Draft"):
#         with st.spinner("Generating complaint draft..."):
#             try:
#                 draft = generate_complaint_from_message(st.session_state.last_prompt)
#                 st.success("âœ… Complaint draft generated:")
#                 st.code(draft, language="markdown")
#             except Exception as e:
#                 st.error(f"âŒ Draft generation failed: {e}")





# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI
# from langchain.schema import HumanMessage

# # API Setup
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"  # Replace with your actual key

# # Initialize the LLM
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # Streamlit config
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")



# # Session state setup
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# if "last_prompt" not in st.session_state:
#     st.session_state.last_prompt = ""
# if "show_draft_button" not in st.session_state:
#     st.session_state.show_draft_button = False
# if "draft_generated" not in st.session_state:
#     st.session_state.draft_generated = False

# # Display previous messages
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # Handle new prompt
# if prompt := st.chat_input("Ask a legal question..."):
#     st.session_state.last_prompt = prompt
#     st.session_state.show_draft_button = False
#     st.session_state.draft_generated = False

#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)
#             st.session_state.messages.append({"role": "assistant", "content": response.content})
#             st.session_state.show_draft_button = True
#         except Exception as e:
#             st.error(f"âŒ Chat Error: {e}")

# # Complaint Draft Button (if assistant already replied)
# if st.session_state.show_draft_button and not st.session_state.draft_generated:
#     if st.button("ğŸ“„ Generate Complaint Draft"):
#         with st.spinner("Generating complaint draft..."):
#             try:
#                 draft = generate_complaint_from_message(st.session_state.last_prompt)
#                 st.session_state.draft_generated = True
#                 st.session_state.messages.append({"role": "assistant", "content": draft})
#                 with st.chat_message("assistant"):
#                     st.success("âœ… Complaint draft generated!")
#                     st.markdown(draft)
#             except Exception as e:
#                 st.error(f"âŒ Error generating draft: {e}")






# import warnings
# warnings.filterwarnings("ignore", category=DeprecationWarning)

# import os
# import streamlit as st
# from generate_complaint import generate_complaint_from_message
# from langchain_openai import ChatOpenAI
# from langchain.schema import HumanMessage

# # ğŸ” Set OpenRouter credentials
# os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
# os.environ["OPENAI_API_KEY"] = "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"  # Replace with your OpenRouter key

# # ğŸ”— Initialize LLM
# llm = ChatOpenAI(
#     model_name="mistralai/mixtral-8x7b-instruct",
#     temperature=0.6
# )

# # ğŸŒ Streamlit UI config
# st.set_page_config(page_title="LawMate AI Chatbot", layout="centered")
# st.title("âš–ï¸ LawMate - AI Legal Assistant")

# # ğŸ—‚ Complaint Type Selector
# complaint_types = [
#     "Consumer Complaint",
#     "Rent Dispute",
#     "FIR / Police Complaint",
#     "Workplace Harassment",
#     "Refund Request"
# ]

# selected_type = st.selectbox("Choose Complaint Type", complaint_types)
# st.session_state["complaint_type"] = selected_type  # âœ… Store in session state

# # ğŸ”„ Session state for chat
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # ğŸ§¾ Display previous messages
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         st.markdown(msg["content"])

# # ğŸ§  Main chat interaction
# if prompt := st.chat_input("Ask a legal question..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})

#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         try:
#             response = llm.invoke([HumanMessage(content=prompt)])
#             st.markdown(response.content)

#             # âœ… Complaint Draft Generation
#             if st.button("ğŸ“„ Generate Complaint Draft"):
#                 with st.spinner("Generating draft..."):
#                     try:
#                         draft = generate_complaint_from_message(prompt)
#                         st.success("âœ… Draft generated!")
#                         st.markdown("### ğŸ“„ Complaint Draft:")
#                         st.code(draft, language="markdown")
#                     except Exception as e:
#                         st.error(f"âŒ Error generating draft: {e}")

#         except Exception as e:
#             st.error(f"âŒ Error: {e}")

#     st.session_state.messages.append({"role": "assistant", "content": response.content})



